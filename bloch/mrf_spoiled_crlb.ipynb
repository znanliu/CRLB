{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d55dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "%run mrf_sequence_epg.ipynb\n",
    "%run PerlinNoise.ipynb\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6934aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crb_mrf_objective(np_in, M0, TEs, inversion_delay, T1, T2, print_matrix=False, apply_trace=True):\n",
    "    in_split = np.split(np_in, 2)\n",
    "    FAs = in_split[0]\n",
    "    TRs = in_split[1]\n",
    "    \n",
    "    m_echos, dm_dM0, dm_dT1, dm_dT2 = mrf_ir_fisp_efficient_crb_forward_differentiation(M0, FAs, TEs, TRs, inversion_delay, T1, T2)\n",
    "    params_to_consider = (dm_dM0, dm_dT1, dm_dT2)\n",
    "    \n",
    "    p = len(params_to_consider)\n",
    "    \n",
    "    crb_combined = calculate_crb_for_tissue(params_to_consider)    \n",
    "\n",
    "    W = np.eye(p)# weighting matrix for calculating trace    \n",
    "    \n",
    "    do_relative_crb = True\n",
    "    \n",
    "    crb_weighting = 1e2 # multiply because higher scalar value on the objective seems to help convergence\n",
    "    \n",
    "    if(do_relative_crb):\n",
    "        # don't need to respect the autograd conventions here, since the reverse  mode ad will backpropagate through\n",
    "        # this weighting which is not used in the CRLB calculation\n",
    "        W[0, 0] = 1\n",
    "        W[1, 1] = 1/np.square(T1)\n",
    "        W[2, 2] = 1/np.square(T2)\n",
    "    else:\n",
    "        # this weighting puts all values roughly on the same scale, copy from zhao et al\n",
    "        W[0, 0] = 3e1 # consider M0\n",
    "        W[1, 1] = 2e-5 # consider T1\n",
    "        W[2, 2] = 5e-4 # consider T2     \n",
    "        \n",
    "    if(print_matrix):\n",
    "        print(W * crb_combined * crb_weighting)\n",
    "    \n",
    "    if(apply_trace): # use this for optimization\n",
    "        return np.trace(W * crb_combined) * crb_weighting\n",
    "    else: # use this for plotting objectives separately\n",
    "        return np.diag(W * crb_combined) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e8700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelized finite differences over crb_mrf_objective\n",
    "def crb_mrf_grad_parallel(np_in, *args):\n",
    "    \n",
    "    N_processes = 16\n",
    "    M0, TEs, inversion_delay, T1, T2, opt_meta_data, mask_tuple = args\n",
    "\n",
    "    if (mask_tuple is None):\n",
    "        x_in = np_in\n",
    "    else:\n",
    "        x_in = np.copy(mask_tuple[0])\n",
    "        x_in[mask_tuple[1]] = np_in\n",
    "        \n",
    "    crb_mrf_objective_partial = partial(crb_mrf_objective, M0=M0, TEs=TEs, inversion_delay=inversion_delay, T1=T1, T2=T2)\n",
    "    \n",
    "    N_vars = np.size(x_in)\n",
    "    N_vars_to_optimize = np.size(np_in)\n",
    "    f_x0 = crb_mrf_objective_partial(x_in)\n",
    "    # same value as numerical approx when fgrad is not provided, enable this for exact same result as if fprime is not provided to fmin\n",
    "    #step_size = 1.4901161193847656e-08 \n",
    "    step_size = 1e-4\n",
    "       \n",
    "    x_in_plus_df = np.repeat(x_in[np.newaxis, :], N_vars, axis=0) + np.eye(N_vars) * step_size       \n",
    "\n",
    "    if (mask_tuple is not None):\n",
    "        x_in_plus_df = x_in_plus_df[mask_tuple[1], :]\n",
    "\n",
    "    with Pool(N_processes) as p: # since we have relatively few iterations, just create the pool each time grad is called\n",
    "        f_x_plus_df = p.map(crb_mrf_objective_partial, x_in_plus_df)    \n",
    "\n",
    "    df_dxi = (f_x_plus_df - f_x0) / step_size\n",
    "        \n",
    "    if(opt_meta_data is not None):\n",
    "        opt_meta_data.increment_iteration_number()\n",
    "    \n",
    "    # save iterations\n",
    "    save_to_temp_log_file(x_in, f_x0)\n",
    "        \n",
    "    return df_dxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae0556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
