{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e695c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import autograd.numpy as np\n",
    "\n",
    "from autograd.core import make_vjp as _make_vip, make_jvp as _make_jvp\n",
    "from autograd.extend import primitive, defvjp_argnum, vspace\n",
    "from autograd.wrap_util import unary_to_nary\n",
    "from autograd.builtins import tuple as atuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1426c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T1_index(x):\n",
    "    return get_arg_index(x, 'T1')\n",
    "\n",
    "def get_T2_index(x):\n",
    "    return get_arg_index(x, 'T2')\n",
    "\n",
    "def get_df_index(x):\n",
    "    return get_arg_index(x, 'df')\n",
    "\n",
    "def get_M0_index(x):\n",
    "    return get_arg_index(x, 'M0')\n",
    "\n",
    "def get_arg_index(x, var_name):\n",
    "    return list(inspect.signture(x).parameters.key()).index(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49998e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@unary_to_nary\n",
    "def jacobian_pkl(fun, x):\n",
    "    vjp, ans = _make_vjp(fun, x)\n",
    "    ans_vspace = vspace(ans)\n",
    "    jacobian_shape = ans_vspace.shape + vspace(x).shape\n",
    "    grads = map(vjp, ans_vspace.standard_basis())\n",
    "    \n",
    "    grads_out = np.stack(grads)\n",
    "    \n",
    "    if(np.prod(jacobian_shape) == np.prod(grads_out.shape)):\n",
    "        return np.reshape(grads_oout, jacobian_shape)\n",
    "    else:\n",
    "        my_jacobian_shape = ans_vspace.shape + vspace(x).shape + (2, )\n",
    "        re_im_grads = np.squeeze(np,reshape(grads_out, my_jacobian_shape))\n",
    "        out = re_im_grads[..., 0] + 1j*re_im_grads[..., 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272ae031",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Return the numerical gradient of the function g, using central difference\n",
    "\n",
    "@parm g function to calculate gradient of\n",
    "@parm x1 dictionary of arguments to pass to function (point at which to calculate derivative)\n",
    "@parm dg_arg key in dictionary for which to take the derivative wrt\n",
    "'''\n",
    "\n",
    "def numerical_grad(g, x1_in, dg_arg):\n",
    "    x1 = x1_in.copy()\n",
    "    x2 = x1_in.copy()\n",
    "    \n",
    "    step_size = 1e-3*x1[dg_arg]\n",
    "    \n",
    "    x1[dg_arg] = x1[dg_arg] + step_size  # increase x1 by a small step\n",
    "    x2[dg_arg] = x1[dg_arg] - step_size  # decrease x2 by a small step\n",
    "    \n",
    "    y1 = g(**x1)\n",
    "    y2 = g(**x2)\n",
    "    \n",
    "    return (y1-y2) / (2*step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d034a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_2(g, x_in, dg_arg, step_size = 1e-3):\n",
    "    N = x_in.shape[0]\n",
    "    dg_dx = np.zeros(N)\n",
    "    \n",
    "    for ii in range(0, N):\n",
    "        step_vector = np.zeros(N)\n",
    "        step_vector[ii] = step_size\n",
    "        \n",
    "        y1 = g(x_in + step_vector, *dg_arg)\n",
    "        y2 = g(x_in - step_vector, *dg_arg)\n",
    "        \n",
    "        dg_dx[ii] = (y1-y2) / (2*step_size)\n",
    "        \n",
    "    return dg_dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b697db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accepts derivatives of M_echo wrt a parameter, should be a tuple list, where each element of the tuple\n",
    "# is a numpy array of size N x 2, and p is the number of parameters in the tuple\n",
    "#\n",
    "# N is number of echoes and 2 is the x/y components of the derivative\n",
    "def calculate_crb_for_tissue(J_n_tuple):        \n",
    "        \n",
    "    J_n = np.dstack(J_n_tuple)\n",
    "    \n",
    "    N, xy_comps, p = J_n.shape\n",
    "    \n",
    "    assert(xy_comps == 2)\n",
    "    \n",
    "    #I_n = np.matmul(np.transpose(J_n, (0, 2, 1)), J_n) # I_n is size (N x p x p) # ideally would use this\n",
    "    \n",
    "    # we loop over N because matmul is not supported for nested object arrays if we are trying to differentiate trace of crb\n",
    "    I_n = []\n",
    "    \n",
    "    for ii in range(0, N):\n",
    "        I_n.append(np.dot(np.transpose(J_n[ii, :, :]), J_n[ii, :, :]))\n",
    "    \n",
    "    I = np.sum(np.array(I_n), axis=0) # sum over echos\n",
    "    \n",
    "    #def matrix_inv_fun(A): # this won't work for nested derivatives \n",
    "    #    return np.linalg.inv(A)\n",
    "    \n",
    "    def matrix_inv_fun_1x1(A):\n",
    "        return 1./A\n",
    "    \n",
    "    def matrix_inv_fun_2x2(A): # this is analytical solution for 2x2\n",
    "        # np.linalg does not support inverse for I, when it is full of autograd boxes, so we resort to the analytical inverse\n",
    "        a, b, c, d = (A[0, 0], A[0, 1], A[1, 0], A[1, 1])\n",
    "        det_A = a * d - b * c\n",
    "        return (1. / det_A) * np.array([[d, -b], [-c, a]])    \n",
    "\n",
    "    def matrix_inv_fun_3x3(A): # analytical solution for 3x3, only compute diagonal elements to save some computation\n",
    "        # https://ardoris.wordpress.com/2008/07/18/general-formula-for-the-inverse-of-a-3x3-matrix/\n",
    "        #a, b, c, d, e, f, g, h, i = A[:]\n",
    "        a, b, c, d, e, f, g, h, i = (A[0, 0], A[0, 1], A[0, 2], A[1, 0], A[1, 1], A[1, 2], A[2, 0], A[2, 1], A[2, 2])\n",
    "        det_A = a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n",
    "        #mat = np.array([[e * i - f * h, c * h - b * i, b * f - c * e],\n",
    "        #               [f * g - d * i, a * i - c * g, c * d - a * f],\n",
    "        #               [d * h - e * g, b * g - a * h, a * e - b * d]])\n",
    "        \n",
    "        # have to be careful to wrap inside np array to maintain autograd status\n",
    "        mat = np.diag(np.array([e * i - f * h, a * i - c * g, a * e - b * d]))\n",
    "        \n",
    "        return (1./ det_A) * mat\n",
    "\n",
    "    # http://www.cs.nthu.edu.tw/~jang/book/addenda/matinv/matinv/\n",
    "    def matrix_inv_fun_4x4(A_in): # could get away with only calculating diagonal elements...\n",
    "\n",
    "        A = A_in[0:3, 0:3]\n",
    "        c = A_in[3, 3]\n",
    "        b = A_in[0:3, 3][:, np.newaxis]\n",
    "\n",
    "        k = c - np.dot(np.dot(np.transpose(b), matrix_inv_fun_3x3(A)), b)\n",
    "        A_inv_00 = matrix_inv_fun_3x3(A - np.dot(b, np.transpose(b))/c )\n",
    "        A_inv_01 = -1 / k * np.dot(matrix_inv_fun_3x3(A), b)\n",
    "        A_inv_11 = 1/k\n",
    "\n",
    "        A_inv_tmp_1 = np.concatenate((A_inv_00, A_inv_01), axis=1)\n",
    "        A_inv_tmp_2 = np.concatenate((np.transpose(A_inv_01), A_inv_11), axis=1)\n",
    "        A_inv = np.concatenate((A_inv_tmp_1, A_inv_tmp_2), axis=0)\n",
    "\n",
    "        return A_inv\n",
    "    \n",
    "    if(p == 2):\n",
    "        matrix_inv_fun = matrix_inv_fun_2x2\n",
    "    elif(p == 3):\n",
    "        matrix_inv_fun = matrix_inv_fun_3x3\n",
    "    elif(p == 4):\n",
    "        matrix_inv_fun = matrix_inv_fun_4x4\n",
    "    else:    \n",
    "        matrix_inv_fun = matrix_inv_fun_1x1\n",
    "        \n",
    "        \n",
    "    crb = matrix_inv_fun(I)\n",
    "    \n",
    "    return crb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662f73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have a simple cache struct because in the tensorflow, the gradient and the objective are computed simultaneously\n",
    "# so we can recycle the values\n",
    "class CacheStruct:\n",
    "    _cache = ()      # cached values stored in tuple\n",
    "    _previous_x = 0. # key for accessing cache\n",
    "    _num_cache_hits = 0 # just to see if the cache is working/ how much time is saved\n",
    "    _verbose = False\n",
    "    _rtol = 1e-5\n",
    "    \n",
    "    def __init__(self, is_verbose=False, cache_exact=False):\n",
    "        self._verbose = is_verbose\n",
    "            \n",
    "        # cache_exact set to True means that the numpy arrays must match within very strict\n",
    "        # relative tolerance. This reduces the number of cache hits but is more exact\n",
    "        if(cache_exact):\n",
    "            self._rtol = 1e-10        \n",
    "    \n",
    "    def check_cache(self, curr_x):                \n",
    "        result = np.allclose(curr_x, self._previous_x, rtol=self._rtol)\n",
    "        \n",
    "        if(self._verbose):\n",
    "            print('check_cache')            \n",
    "            if(result):\n",
    "                print(curr_x)\n",
    "                print(self._previous_x)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache(self):\n",
    "        if(self._verbose):\n",
    "            print('get_cache')\n",
    "            \n",
    "        self._num_cache_hits = self._num_cache_hits + 1  # assumes that getting cache corresponds to hit\n",
    "        return self._cache\n",
    "     \n",
    "    # @param new_x is numpy array\n",
    "    def update_cache(self, new_x, new_cache):\n",
    "        self._previous_x = np.array(new_x) # have to copy the array otherwise the cache fails\n",
    "        self._cache = new_cache\n",
    "        \n",
    "    def get_cache_hits(self):\n",
    "        return self._num_cache_hits    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58f517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple class that holds metadata on the optimization   \n",
    "class OptimizationMetaData:\n",
    "    _iter = 0.\n",
    "    \n",
    "    def __init__(self):\n",
    "        _iter = 0.\n",
    "        \n",
    "    def get_iteration_number(self):\n",
    "        return self._iter\n",
    "    \n",
    "    def increment_iteration_number(self):\n",
    "        self._iter = self._iter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2c555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
